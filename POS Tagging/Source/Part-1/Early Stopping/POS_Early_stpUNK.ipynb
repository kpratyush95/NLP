{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Early_stpUNK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG7G4hHOAwev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VREDyAcR7xO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From utils.py\n",
        "def read_data(f):\n",
        "\twith open(f) as inp:\n",
        "\t\tlines = inp.readlines()\n",
        "\tdata = []\n",
        "\tfor line in lines:\n",
        "\t\tline = line.strip().split()\n",
        "\t\tsentence = []\n",
        "\t\tfor token in line:\n",
        "\t\t\ttoken = token.split('|')\n",
        "\t\t\tword = token[0]\n",
        "\t\t\ttag = token[1]\n",
        "\t\t\tsentence.append((word,tag))\n",
        "\t\tdata.append(sentence)\n",
        "\treturn data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7fMMx9f8ezG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from utils.py\n",
        "def convert_data_for_training(data):\n",
        "\t#for d in data:\n",
        "\t#\ttokens = [t[0] for t in d]\n",
        "\t#\ttags = [t[1] for t in d]\n",
        "\treturn [([t[0] for t in d],[t[1] for t in d]) for d in data]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O_WVWcQ8l39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pytorch_tagging.py\n",
        "TRAINING_FILE = \"./irish.train\"\n",
        "training_data = convert_data_for_training(read_data(TRAINING_FILE))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NQC5TVn5HJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_FILE = \"./irish.dev\"\n",
        "validation_data = convert_data_for_training(read_data(VALIDATION_FILE))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUiD4gxv_DYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from utils.py\n",
        "def rareWordstoUNK(tokenlist,keeplist, replaceToken='UNK'):\n",
        "\treturn [w if w in keeplist else replaceToken for w in tokenlist]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgkP_K0a8tH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from utils.py\n",
        "def substitute_with_UNK(data, n=1):\n",
        "\tword_frequency = {}\n",
        "\tdata_list = pd.DataFrame(data)\n",
        "\ttoken_list = data_list[0]\n",
        "\tfor token in token_list:\n",
        "\t\tfor word in token:\n",
        "\t\t\tif word in word_frequency:\n",
        "\t\t\t\tword_frequency[word]=word_frequency[word]+1\n",
        "\t\t\telse:\n",
        "\t\t\t\tword_frequency[word]=1\n",
        "\tfrequent_words=[key for key,value in word_frequency.items() if value >1]\n",
        "\ttoken_list = token_list.apply(rareWordstoUNK, args=(frequent_words,'UNK'))\n",
        "\treturn list(zip(token_list,pd.DataFrame(data)[1]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWe0r7X5_EIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pytorch_taggin.py\n",
        "# training_data_UNK = utils.substitute_with_UNK(training_data)\n",
        "training_data_UNK = substitute_with_UNK(training_data)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26YrTKdK74_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validataion_data_UNK = substitute_with_UNK(validation_data)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qVh1mDZA9Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pytorch_tagging.py \n",
        "def words_tags_indes(data):\n",
        "\tword_to_ix = {}#{'UNK':0}\n",
        "\tix_to_word = {}#{0:'UNK'}\n",
        "\ttag_to_ix = {}\n",
        "\tix_to_tag = {}\n",
        "\tfor sent, tags in data:\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word not in word_to_ix:\n",
        "\t\t\t\tword_to_ix[word] = len(word_to_ix)\n",
        "\t\t\t\tix_to_word[word_to_ix[word]] = word\n",
        "\t\tfor tag in tags:\n",
        "\t\t\tif tag not in tag_to_ix:\n",
        "\t\t\t\ttag_to_ix[tag] = len(tag_to_ix)\n",
        "\t\t\t\tix_to_tag[tag_to_ix[tag]] = tag\n",
        "\treturn word_to_ix,ix_to_word,ix_to_tag, tag_to_ix"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASFeabbyItKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix_UNK, ix_to_word_UNK, ix_to_tag_UNK, tag_to_ix_UNK = words_tags_indes(training_data_UNK)\n",
        "#valid_word_to_ix_UNK, valid_ix_to_word_UNK, valid_ix_to_tag_UNK, valid_tag_to_ix_UNK = words_tags_indes(validataion_data_UNK)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7yhDw-dBNdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "torch.manual_seed(42)\n",
        "#Converts a sequence of words to a tensor of numerical values. \n",
        "def prepare_sequence(seq, to_ix):\n",
        "\tidxs = []\n",
        "\tfor word in seq:\n",
        "\t\tif word in to_ix:\n",
        "\t\t\tidxs.append(to_ix[word])\n",
        "\t\telse:\n",
        "\t\t\t idxs.append(to_ix['UNK'])\n",
        "\treturn torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo1TMumUBuoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "class LSTMTagger(nn.Module):\n",
        "\t# Class that defines our model\n",
        "\tdef __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "\t\tsuper(LSTMTagger, self).__init__()\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "\t\t# The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "\t\t# with dimensionality hidden_dim.\n",
        "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim)#, bidirectional=True)\n",
        "\n",
        "\t\t# The linear layer that maps from hidden state space to tag space\n",
        "\t\tself.hidden2tag = nn.Linear(hidden_dim, tagset_size) #nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "\t# This is the forward computation, which constructs the computation graph\n",
        "\tdef forward(self, sentence):\n",
        "\t\t# Get the embeddings\n",
        "\t\tembeds = self.word_embeddings(sentence)\n",
        "\t\t# put them through the LSTM and get its output\n",
        "\t\tlstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "\t\t# pass that output through the linnear layer\n",
        "\t\ttag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "\t\t# convert the logits to a log probability distribution\n",
        "\t\ttag_scores = F.log_softmax(tag_space, dim=1)\n",
        "\t\treturn tag_scores"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4jz-UCrB8t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 32\n",
        "# DROPOUT = ?\n",
        "# LAYERS = ?"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsaWzD9VCBkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_taggin.py\n",
        "# Initialize the model\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix_UNK), len(tag_to_ix_UNK))\n",
        "# Loss function to use\n",
        "loss_function = nn.NLLLoss()\n",
        "# Optimizer to use during training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8cSfopJ8vq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_taggin.py\n",
        "# Training loop\n",
        "def train_model(model,n_epochs, patience, training_data_UNK, validation_data):\n",
        "\tpatience_counter = 0\n",
        "\tmin_loss = np.inf\n",
        "\tpath = \"./model_save_Early_stop_UNK.pth\"\n",
        "\tavg_training_losses = []\n",
        "\tavg_valid_losses = []\n",
        "\tfor epoch in range(50):  # normally you would NOT do 100 epochs, it is toy data\n",
        "\t\tprint(f\"Starting epoch {epoch}...\")\n",
        "\t\ttraining_losses = []\n",
        "\t\tfor sentence, tags in training_data_UNK:\n",
        "\t\t\t# Step 1. Remember that Pytorch accumulates gradients.\n",
        "\t\t\t# We need to clear them out before each instance\n",
        "\t\t\tmodel.zero_grad()\n",
        "\n",
        "\t\t\t# Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "\t\t\t# Tensors of word indices.\n",
        "\t\t\t# Eventually I suggest you use the DataLoader modules\n",
        "\t\t\t# The batching can take place here\n",
        "\t\t\tsentence_in = prepare_sequence(sentence, word_to_ix_UNK)\n",
        "\t\t\ttargets = prepare_sequence(tags, tag_to_ix_UNK)\n",
        "\t\t\t# Step 3. Run our forward pass.\n",
        "\t\t\ttag_scores = model(sentence_in)\n",
        "\t\n",
        "\t\t\t# Step 4. Compute the loss, gradients, and update the parameters by\n",
        "\t\t\t#  calling optimizer.step()\n",
        "\t\t\tloss = loss_function(tag_scores, targets)\n",
        "\t\t\ttraining_losses.append(loss.item())\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\tavg_training_losses.append(np.average(training_losses))\n",
        "\t\tprint(\" Training Loss is \",avg_training_losses[epoch])\n",
        "\t\tvalid_losses = []\n",
        "\t\tfor validation_sentence, validation_tags in validation_data:\n",
        "\t\t\tvalidation_sentence_in = prepare_sequence(validation_sentence, word_to_ix_UNK)\n",
        "\t\t\tvalidation_targets = prepare_sequence(validation_tags, tag_to_ix_UNK)\n",
        "\t\t\tvalidation_tag_scores = model(validation_sentence_in)\n",
        "\t\t\tvalidation_loss = loss_function(validation_tag_scores, validation_targets)\n",
        "\t\t\tvalid_losses.append(validation_loss.item())\n",
        "\t\tavg_valid_losses.append(np.average(valid_losses))\n",
        "\t\tprint(\"validation loss is \",avg_valid_losses[epoch])\n",
        "\t\tif min_loss > avg_valid_losses[epoch]:\n",
        "\t\t\tpatience_counter = 0\n",
        "\t\t\tmin_loss = avg_valid_losses[epoch]\n",
        "\t\telse:\n",
        "\t\t\tpatience_counter+=1\n",
        "\t\t\tif patience_counter == patience:\n",
        "\t\t\t\tprint(\"Early Stopping to avoid overfitting\")\n",
        "\t\t\t\tmodel_checkpoint = torch.load(path)\n",
        "\t\t\t\tmodel.load_state_dict(model_checkpoint['state_dict'])\n",
        "\t\t\t\toptimizer.load_state_dict(model_checkpoint['optimizer'])\n",
        "\t\t\t\tmodel_epoch = model_checkpoint['epoch']\n",
        "\t\t\t\tprint(\"patience counter is \",patience_counter)\n",
        "\t\t\t\tprint(\"loaded epoch is\",model_epoch)\n",
        "\t\t\t\tbreak\n",
        "\t\tif patience_counter==0:\n",
        "\t\t\tprint(\"Saving model here\")\n",
        "\t\t\tmodel_state = {'epoch' : epoch,\n",
        "\t\t\t\t\t\t'state_dict' : model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer' : optimizer.state_dict(),\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\ttorch.save(model_state, path)\n",
        "\t\tprint(patience_counter)\n",
        "\treturn avg_training_losses, avg_valid_losses"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWEm3aOz7UVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d1afa25-cbd4-4125-82b4-87cf637603e0"
      },
      "source": [
        "avg_training_losses, avg_valid_losses = train_model(model,20, 8, training_data, validation_data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 0...\n",
            " Training Loss is  1.4683337459371135\n",
            "validation loss is  1.0803982592342698\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 1...\n",
            " Training Loss is  0.9308210200666569\n",
            "validation loss is  0.8611823955092356\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 2...\n",
            " Training Loss is  0.7612525706235717\n",
            "validation loss is  0.7616524794727498\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 3...\n",
            " Training Loss is  0.6686052944033289\n",
            "validation loss is  0.7042176538578621\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 4...\n",
            " Training Loss is  0.6056271723012955\n",
            "validation loss is  0.6658569554284116\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 5...\n",
            " Training Loss is  0.5589162792760287\n",
            "validation loss is  0.6375619114238248\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 6...\n",
            " Training Loss is  0.5220722689597855\n",
            "validation loss is  0.6162899442764614\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 7...\n",
            " Training Loss is  0.4916458731131624\n",
            "validation loss is  0.5998148894631314\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 8...\n",
            " Training Loss is  0.46599069992539016\n",
            "validation loss is  0.5868641587765164\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 9...\n",
            " Training Loss is  0.44370498990132723\n",
            "validation loss is  0.5766603614754992\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 10...\n",
            " Training Loss is  0.4240452255955412\n",
            "validation loss is  0.5687231250559527\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 11...\n",
            " Training Loss is  0.4064611682740497\n",
            "validation loss is  0.5623803415842676\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 12...\n",
            " Training Loss is  0.3907580843276461\n",
            "validation loss is  0.5571844215022643\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 13...\n",
            " Training Loss is  0.37667674840343435\n",
            "validation loss is  0.5528629326515577\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 14...\n",
            " Training Loss is  0.36389375811013147\n",
            "validation loss is  0.5491834821372072\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 15...\n",
            " Training Loss is  0.3521862858177171\n",
            "validation loss is  0.5456568496510853\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 16...\n",
            " Training Loss is  0.34134267838735516\n",
            "validation loss is  0.5434079907352865\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 17...\n",
            " Training Loss is  0.33135093599863097\n",
            "validation loss is  0.5420758757281309\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 18...\n",
            " Training Loss is  0.32211237876087706\n",
            "validation loss is  0.5411886902646824\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 19...\n",
            " Training Loss is  0.3135405348389035\n",
            "validation loss is  0.5406981931016894\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 20...\n",
            " Training Loss is  0.3055604228359195\n",
            "validation loss is  0.5404952185748851\n",
            "Saving model here\n",
            "0\n",
            "Starting epoch 21...\n",
            " Training Loss is  0.2980921222516084\n",
            "validation loss is  0.5405332606906348\n",
            "1\n",
            "Starting epoch 22...\n",
            " Training Loss is  0.2910832090738387\n",
            "validation loss is  0.5408167098251331\n",
            "2\n",
            "Starting epoch 23...\n",
            " Training Loss is  0.2844809540001031\n",
            "validation loss is  0.5413675271741274\n",
            "3\n",
            "Starting epoch 24...\n",
            " Training Loss is  0.27823698257578405\n",
            "validation loss is  0.5421670157095302\n",
            "4\n",
            "Starting epoch 25...\n",
            " Training Loss is  0.2723212166466794\n",
            "validation loss is  0.5432074260750802\n",
            "5\n",
            "Starting epoch 26...\n",
            " Training Loss is  0.2666610283915821\n",
            "validation loss is  0.5444811148281951\n",
            "6\n",
            "Starting epoch 27...\n",
            " Training Loss is  0.26123296130143964\n",
            "validation loss is  0.5459870948048903\n",
            "7\n",
            "Starting epoch 28...\n",
            " Training Loss is  0.256055632592341\n",
            "validation loss is  0.5480250850089613\n",
            "Early Stopping to avoid overfitting\n",
            "patience counter is  8\n",
            "loaded epoch is 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSAhtzQJ7jvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ac3b734a-c86c-42df-99ba-e8e4eb2215ba"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(avg_training_losses, label = 'Train')\n",
        "plt.plot(avg_valid_losses, label = 'Validation')\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYDE2h_KCNPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# #pytorch_tagging.py\n",
        "# # See what the scores are before training\n",
        "# # Note that element i,j of the output is the score for tag j for word i.\n",
        "# # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "# with torch.no_grad():\n",
        "#   #changes here\n",
        "#   for index in range(len(training_data_UNK)):\n",
        "#     #changes here\n",
        "# \t  inputs = prepare_sequence(training_data_UNK[index][0], word_to_ix_UNK)\n",
        "# \t  tag_scores = model(inputs)\n",
        "# \t  print(tag_scores)\n",
        "#    #changes here\n",
        "# \t  for i,word in enumerate(training_data_UNK[index][0]):\n",
        "# \t\t  j = int(np.argmax(tag_scores[i]))\n",
        "# \t\t  print(f\"\\t{word}|{ix_to_tag_UNK[j]}\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLLqfC_ZWXnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_FILE = \"./irish.test\"\n",
        "test_data = convert_data_for_training(read_data(TEST_FILE))\n",
        "\n",
        "with torch.no_grad():\n",
        "\t# this will be the file to write the outputs\n",
        "\twith open(\"mymodel_output_irish_UNK.txt\", 'w') as op:\n",
        "\t\tfor instance in test_data:\n",
        "\t\t\t# Convert the test sentence into a word ID tensor\n",
        "\t\t\tinputs = prepare_sequence(instance[0], word_to_ix_UNK)\n",
        "\t\t\t# Forward pass\n",
        "\t\t\ttag_scores = model(inputs)\n",
        "\t\t\t# Find the tag with the highest probability in each position\n",
        "\t\t\toutputs = [int(np.argmax(ts)) for ts in tag_scores]\n",
        "\t\t\t# Prepare the output to be written in the same format as the test file (word|tag)\n",
        "\t\t\tformatted_output = ' '.join([f\"{word}|{ix_to_tag_UNK[tag_id]}\" for word,tag_id in zip(instance[0],outputs)])\n",
        "\t\t\t# Write the output\n",
        "\t\t\top.write(formatted_output + '\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IjC0sTZCYz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute_accuracy\n",
        "def acc_read_data(f):\n",
        "\twith open(f) as inp:\n",
        "\t\tlines = inp.readlines()\n",
        "\tdata = []\n",
        "\tfor line in lines:\n",
        "\t\tline = line.strip().split()\n",
        "\t\tsentence = []\n",
        "\t\tfor token in line:\n",
        "\t\t\ttoken = token.split('|')\n",
        "\t\t\tword = token[0]\n",
        "\t\t\ttag = token[1]\n",
        "\t\t\tsentence.append((word,tag))\n",
        "\t\tdata.append(sentence)\n",
        "\treturn data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GISb8RaMTfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(output, gold):\n",
        "\ttry:\n",
        "\t\tassert(len(output) == len(gold))\n",
        "\texcept:\n",
        "\t\tprint(\"Different number of lines in the two files!\")\n",
        "\t\treturn -1\n",
        "\n",
        "\tcount_correct = 0\n",
        "\tcount_total_tokens = 0\n",
        "\tfor o_sent,g_sent in zip(output,gold):\n",
        "\t\ttry:\n",
        "\t\t\tassert(len(o_sent)==len(g_sent))\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Different number of tokens in the two lines!\")\n",
        "\t\t\treturn -1\n",
        "\t\tcheck = [o_token[1] == g_token[1] for o_token,g_token in zip(o_sent,g_sent)]\n",
        "\t\tcount_correct += sum(check)\n",
        "\t\tcount_total_tokens += len(check)\n",
        "\treturn count_correct/count_total_tokens"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi4LzYzIMhrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = acc_read_data(\"./mymodel_output_irish_UNK.txt\")\n",
        "gold = acc_read_data(\"./irish.test\")\n",
        "acc = compute_accuracy(output,gold)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGqv7_XNNJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db91a2b1-aa34-4b2d-eb32-87552ee28994"
      },
      "source": [
        "acc"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8336136116331981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNL5iAweZaa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}