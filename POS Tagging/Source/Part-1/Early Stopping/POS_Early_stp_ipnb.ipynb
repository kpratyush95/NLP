{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_Early_stp.ipnb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUAObcsQWCxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8l45lPXWHhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From utils.py\n",
        "def read_data(f):\n",
        "\twith open(f) as inp:\n",
        "\t\tlines = inp.readlines()\n",
        "\tdata = []\n",
        "\tfor line in lines:\n",
        "\t\tline = line.strip().split()\n",
        "\t\tsentence = []\n",
        "\t\tfor token in line:\n",
        "\t\t\ttoken = token.split('|')\n",
        "\t\t\tword = token[0]\n",
        "\t\t\ttag = token[1]\n",
        "\t\t\tsentence.append((word,tag))\n",
        "\t\tdata.append(sentence)\n",
        "\treturn data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pV8p1hEWHvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from utils.py\n",
        "def convert_data_for_training(data):\n",
        "\t#for d in data:\n",
        "\t#\ttokens = [t[0] for t in d]\n",
        "\t#\ttags = [t[1] for t in d]\n",
        "\treturn [([t[0] for t in d],[t[1] for t in d]) for d in data]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IafhDcnIWH1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pytorch_tagging.py\n",
        "TRAINING_FILE = \"./irish.train\"\n",
        "training_data = convert_data_for_training(read_data(TRAINING_FILE))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGHoxrBg36sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALIDATION_FILE = \"./irish.dev\"\n",
        "validation_data = convert_data_for_training(read_data(VALIDATION_FILE))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp2iFI28WH4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pytorch_tagging.py \n",
        "def words_tags_indes(data):\n",
        "\tword_to_ix = {'UNK':0}\n",
        "\tix_to_word = {0:'UNK'}\n",
        "\ttag_to_ix = {}\n",
        "\tix_to_tag = {}\n",
        "\tfor sent, tags in data:\n",
        "\t\tfor word in sent:\n",
        "\t\t\tif word not in word_to_ix:\n",
        "\t\t\t\tword_to_ix[word] = len(word_to_ix)\n",
        "\t\t\t\tix_to_word[word_to_ix[word]] = word\n",
        "\t\tfor tag in tags:\n",
        "\t\t\tif tag not in tag_to_ix:\n",
        "\t\t\t\ttag_to_ix[tag] = len(tag_to_ix)\n",
        "\t\t\t\tix_to_tag[tag_to_ix[tag]] = tag\n",
        "\treturn word_to_ix,ix_to_word,ix_to_tag, tag_to_ix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qci0Eh7cWH6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix,ix_to_word,ix_to_tag, tag_to_ix = words_tags_indes(training_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mefwnM4HWH93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "torch.manual_seed(42)\n",
        "#Converts a sequence of words to a tensor of numerical values. \n",
        "def prepare_sequence(seq, to_ix):\n",
        "\tidxs = []\n",
        "\tfor word in seq:\n",
        "\t\tif word in to_ix:\n",
        "\t\t\tidxs.append(to_ix[word])\n",
        "\t\telse:\n",
        "\t\t\t idxs.append(to_ix['UNK'])\n",
        "\treturn torch.tensor(idxs, dtype=torch.long)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBPflsz0V9Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "class LSTMTagger(nn.Module):\n",
        "\t# Class that defines our model\n",
        "\tdef __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "\t\tsuper(LSTMTagger, self).__init__()\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "\t\t# The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "\t\t# with dimensionality hidden_dim.\n",
        "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "\t\t# The linear layer that maps from hidden state space to tag space\n",
        "\t\tself.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "\t# This is the forward computation, which constructs the computation graph\n",
        "\tdef forward(self, sentence):\n",
        "\t\t# Get the embeddings\n",
        "\t\tembeds = self.word_embeddings(sentence)\n",
        "\t\t# put them through the LSTM and get its output\n",
        "\t\tlstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "\t\t# pass that output through the linnear layer\n",
        "\t\ttag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "\t\t# convert the logits to a log probability distribution\n",
        "\t\ttag_scores = F.log_softmax(tag_space, dim=1)\n",
        "\t\treturn tag_scores"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfDFR6JnX_bc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_tagging.py\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 32\n",
        "HIDDEN_DIM = 32\n",
        "# DROPOUT = ?\n",
        "# LAYERS = ?"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvjGkvN5YBTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_taggin.py\n",
        "# Initialize the model\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "# Loss function to use\n",
        "loss_function = nn.NLLLoss()\n",
        "# Optimizer to use during training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46kMszuxYH2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch_taggin.py\n",
        "# Training loop\n",
        "def train_model(model,n_epochs, patience, training_data, validation_data):\n",
        "\tpatience_counter = 0\n",
        "\tmin_loss = np.inf\n",
        "\tpath = \"./model_save_Early_stop.pth\"\n",
        "\tavg_training_losses = []\n",
        "\tavg_valid_losses = []\n",
        "\tfor epoch in range(n_epochs):  # normally you would NOT do 100 epochs, it is toy data\n",
        "\t\tprint(f\"Starting epoch {epoch}...\")\n",
        "\t\ttraining_losses = []\n",
        "\t\tfor sentence, tags in training_data:\n",
        "\t\t\t# Step 1. Remember that Pytorch accumulates gradients.\n",
        "\t\t\t# We need to clear them out before each instance\n",
        "\t\t\tmodel.zero_grad()\n",
        "\n",
        "\t\t\t# Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "\t\t\t# Tensors of word indices.\n",
        "\t\t\t# Eventually I suggest you use the DataLoader modules\n",
        "\t\t\t# The batching can take place here\n",
        "\t\t\tsentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "\t\t\ttargets = prepare_sequence(tags, tag_to_ix)\n",
        "\t\t\t# Step 3. Run our forward pass.\n",
        "\t\t\ttag_scores = model(sentence_in)\n",
        "\t\n",
        "\t\t\t# Step 4. Compute the loss, gradients, and update the parameters by\n",
        "\t\t\t#  calling optimizer.step()\n",
        "\t\t\tloss = loss_function(tag_scores, targets)\n",
        "\t\t\ttraining_losses.append(loss.item())\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\tavg_training_losses.append(np.average(training_losses))\n",
        "\t\tprint(\" Training Loss is \",avg_training_losses[epoch])\n",
        "\t\tvalid_losses = []\n",
        "\t\tfor validation_sentence, validation_tags in validation_data:\n",
        "\t\t\tvalidation_sentence_in = prepare_sequence(validation_sentence, word_to_ix)\n",
        "\t\t\tvalidation_targets = prepare_sequence(validation_tags, tag_to_ix)\n",
        "\t\t\tvalidation_tag_scores = model(validation_sentence_in)\n",
        "\t\t\tvalidation_loss = loss_function(validation_tag_scores, validation_targets)\n",
        "\t\t\tvalid_losses.append(validation_loss.item())\n",
        "\t\tavg_valid_losses.append(np.average(valid_losses))\n",
        "\t\tprint(\"validation loss is \",avg_valid_losses[epoch])\n",
        "\t\tif min_loss > avg_valid_losses[epoch]:\n",
        "\t\t\tpatience_counter = 0\n",
        "\t\t\tmin_loss = avg_valid_losses[epoch]\n",
        "\t\telse:\n",
        "\t\t\tpatience_counter+=1\n",
        "\t\t\tif patience_counter == patience:\n",
        "\t\t\t\tprint(\"Early Stopping to avoid overfitting\")\n",
        "\t\t\t\tmodel_checkpoint = torch.load(path)\n",
        "\t\t\t\tmodel.load_state_dict(model_checkpoint['state_dict'])\n",
        "\t\t\t\toptimizer.load_state_dict(model_checkpoint['optimizer'])\n",
        "\t\t\t\tmodel_epoch = model_checkpoint['epoch']\n",
        "\t\t\t\tprint(\"patience counter is \",patience_counter)\n",
        "\t\t\t\tprint(\"loaded epoch is\",model_epoch)\n",
        "\t\t\t\tbreak\n",
        "\t\tif patience_counter==0:\n",
        "\t\t\tprint(\"Saving model here\")\n",
        "\t\t\tmodel_state = {'epoch' : epoch,\n",
        "\t\t\t\t\t\t'state_dict' : model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer' : optimizer.state_dict(),\n",
        "\t\t\t\t\t\t}\n",
        "\t\t\ttorch.save(model_state, path)\n",
        "\t\tprint(\"Patience counter\",patience_counter)\n",
        "\treturn avg_training_losses, avg_valid_losses"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljS4iUE34NSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59db80cc-ff96-4a28-bfbd-c6a110c6d670"
      },
      "source": [
        "avg_training_losses, avg_valid_losses = train_model(model,50, 8, training_data, validation_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 0...\n",
            " Training Loss is  1.565852379855811\n",
            "validation loss is  1.1198798529060348\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 1...\n",
            " Training Loss is  0.9787535238064274\n",
            "validation loss is  0.912713336574529\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 2...\n",
            " Training Loss is  0.8117375570088081\n",
            "validation loss is  0.8184561677888333\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 3...\n",
            " Training Loss is  0.7170460696294736\n",
            "validation loss is  0.7679519505630311\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 4...\n",
            " Training Loss is  0.6516013894792139\n",
            "validation loss is  0.7368716613003393\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 5...\n",
            " Training Loss is  0.6017951272972518\n",
            "validation loss is  0.7140697244463897\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 6...\n",
            " Training Loss is  0.5610748627933444\n",
            "validation loss is  0.6970037990796922\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 7...\n",
            " Training Loss is  0.5264538141665346\n",
            "validation loss is  0.6846218240259426\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 8...\n",
            " Training Loss is  0.49627000048852227\n",
            "validation loss is  0.6760281373575296\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 9...\n",
            " Training Loss is  0.46932911088607976\n",
            "validation loss is  0.6692728595952699\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 10...\n",
            " Training Loss is  0.44483477930570997\n",
            "validation loss is  0.663664001572935\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 11...\n",
            " Training Loss is  0.4223702567313484\n",
            "validation loss is  0.6597127694463122\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 12...\n",
            " Training Loss is  0.40157143267441753\n",
            "validation loss is  0.6573247764168716\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 13...\n",
            " Training Loss is  0.3822385001982786\n",
            "validation loss is  0.6562639097879308\n",
            "Saving model here\n",
            "Patience counter 0\n",
            "Starting epoch 14...\n",
            " Training Loss is  0.36423158823806606\n",
            "validation loss is  0.6569488560171702\n",
            "Patience counter 1\n",
            "Starting epoch 15...\n",
            " Training Loss is  0.3474752148775849\n",
            "validation loss is  0.6587571388961305\n",
            "Patience counter 2\n",
            "Starting epoch 16...\n",
            " Training Loss is  0.331796958852858\n",
            "validation loss is  0.6611742619378596\n",
            "Patience counter 3\n",
            "Starting epoch 17...\n",
            " Training Loss is  0.31703672963513546\n",
            "validation loss is  0.6640876795060957\n",
            "Patience counter 4\n",
            "Starting epoch 18...\n",
            " Training Loss is  0.30319358216760095\n",
            "validation loss is  0.667747619383227\n",
            "Patience counter 5\n",
            "Starting epoch 19...\n",
            " Training Loss is  0.290147299709308\n",
            "validation loss is  0.6722459280574302\n",
            "Patience counter 6\n",
            "Starting epoch 20...\n",
            " Training Loss is  0.27784216614958346\n",
            "validation loss is  0.6774694208051089\n",
            "Patience counter 7\n",
            "Starting epoch 21...\n",
            " Training Loss is  0.26623637987786525\n",
            "validation loss is  0.6833795569919694\n",
            "Early Stopping to avoid overfitting\n",
            "patience counter is  8\n",
            "loaded epoch is 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqEu0h4e6Z0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "5ba5b197-81fd-411f-fc74-425421d2e2d3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(avg_training_losses, label = 'Train')\n",
        "plt.plot(avg_valid_losses, label = 'Validation')\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdjxzRT4YPBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #pytorch_tagging.py\n",
        "# # See what the scores are before training\n",
        "# # Note that element i,j of the output is the score for tag j for word i.\n",
        "# # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "# with torch.no_grad():\n",
        "#   #changes here\n",
        "#   for index in range(len(training_data)):\n",
        "#     #changes here\n",
        "# \t  inputs = prepare_sequence(training_data[index][0], word_to_ix)\n",
        "# \t  tag_scores = model(inputs)\n",
        "# \t  print(tag_scores)\n",
        "#    #changes here\n",
        "# \t  for i,word in enumerate(training_data[index][0]):\n",
        "# \t\t  j = int(np.argmax(tag_scores[i]))\n",
        "# \t\t  print(f\"\\t{word}|{ix_to_tag[j]}\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zF3jlL1Ya2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_FILE = \"./irish.test\"\n",
        "test_data = convert_data_for_training(read_data(TEST_FILE))\n",
        "\n",
        "with torch.no_grad():\n",
        "\t# this will be the file to write the outputs\n",
        "\twith open(\"mymodel_output_irish.txt\", 'w') as op:\n",
        "\t\tfor instance in test_data:\n",
        "\t\t\t# Convert the test sentence into a word ID tensor\n",
        "\t\t\tinputs = prepare_sequence(instance[0], word_to_ix)\n",
        "\t\t\t# Forward pass\n",
        "\t\t\ttag_scores = model(inputs)\n",
        "\t\t\t# Find the tag with the highest probability in each position\n",
        "\t\t\toutputs = [int(np.argmax(ts)) for ts in tag_scores]\n",
        "\t\t\t# Prepare the output to be written in the same format as the test file (word|tag)\n",
        "\t\t\tformatted_output = ' '.join([f\"{word}|{ix_to_tag[tag_id]}\" for word,tag_id in zip(instance[0],outputs)])\n",
        "\t\t\t# Write the output\n",
        "\t\t\top.write(formatted_output + '\\n')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lGyGuHIapNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute_accuracy\n",
        "def acc_read_data(f):\n",
        "\twith open(f) as inp:\n",
        "\t\tlines = inp.readlines()\n",
        "\tdata = []\n",
        "\tfor line in lines:\n",
        "\t\tline = line.strip().split()\n",
        "\t\tsentence = []\n",
        "\t\tfor token in line:\n",
        "\t\t\ttoken = token.split('|')\n",
        "\t\t\tword = token[0]\n",
        "\t\t\ttag = token[1]\n",
        "\t\t\tsentence.append((word,tag))\n",
        "\t\tdata.append(sentence)\n",
        "\treturn data"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ikalXbnbERC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(output, gold):\n",
        "\ttry:\n",
        "\t\tassert(len(output) == len(gold))\n",
        "\texcept:\n",
        "\t\tprint(\"Different number of lines in the two files!\")\n",
        "\t\treturn -1\n",
        "\n",
        "\tcount_correct = 0\n",
        "\tcount_total_tokens = 0\n",
        "\tfor o_sent,g_sent in zip(output,gold):\n",
        "\t\ttry:\n",
        "\t\t\tassert(len(o_sent)==len(g_sent))\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Different number of tokens in the two lines!\")\n",
        "\t\t\treturn -1\n",
        "\t\tcheck = [o_token[1] == g_token[1] for o_token,g_token in zip(o_sent,g_sent)]\n",
        "\t\tcount_correct += sum(check)\n",
        "\t\tcount_total_tokens += len(check)\n",
        "\treturn count_correct/count_total_tokens"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbwiflREYnd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = acc_read_data(\"./mymodel_output_irish.txt\")\n",
        "gold = acc_read_data(\"./irish.test\")\n",
        "acc = compute_accuracy(output,gold)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiO3L0xEajyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "563d65b3-e45a-4f68-e34c-cb9919628cb5"
      },
      "source": [
        "acc"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8002769809081017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9FqWoRb2U-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}